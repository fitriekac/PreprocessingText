{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKNBHFsh1AOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPl9UYo-02M_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from gensim import corpora\n",
        "import pickle\n",
        "import gensim\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords') \n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfq0vjhd08e1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prerprocess(dat):\n",
        "    #Ubah - menjadi space\n",
        "    # untuk kata '\\-' dihapus dan diganti ' '\n",
        "    regex = r'\\-'\n",
        "    dat = dat.str.replace(regex, ' ', regex=True) # hapus\n",
        "    # datat = datat.str.replace(regex, ' ', regex=True) # hapus\n",
        "    regex = r'[^a-zA-Z\\s]' \n",
        "    #selain kata '[^a-zA-Z\\s]' akan dihapus dan diganti menjadi space\n",
        "    # Ubah seluruh huruf dalam data film ke dalam lower case(casefolding)\n",
        "    dat = dat.str.lower() # hapus   \n",
        "    # datat = datat.str.lower()                           (3)\n",
        "\n",
        "    # Hapus seluruh simbol yang ada dan ganti dengan string kosong ('')\n",
        "    dat = dat.str.replace(regex, '', regex=True) # hapus       (4)\n",
        "    # datat = datat.str.replace(regex, '', regex=True) # hapus  \n",
        "    # Hapus enter, ganti dengan space \n",
        "    # untuk kata '\\n' dihapus dan diganti ' ' \n",
        "    regex = r'\\n'\n",
        "    dat = dat.str.replace(regex, ' ', regex=True) # hapus\n",
        "    # datat = datat.str.replace(regex, ' ', regex=True) # hapus\n",
        "    return dat\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "def lemmatize_sentence(sentence):\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:        \n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "def lem(dat):\n",
        "  for i, v in dat.iterrows():\n",
        "    \n",
        "    dat.at[i, 'review']=lemmatize_sentence(v['review'])\n",
        "  return dat\n",
        "\n",
        "def stop(dat):\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "\n",
        "  for i, v in dat.iterrows():\n",
        "    words = word_tokenize(v['review'])\n",
        "    \n",
        "    # dat[i] = [w for w in words if not w in stop_words] \n",
        "      \n",
        "    text=''\n",
        "    for w in words: \n",
        "        if w not in stop_words:\n",
        "          text=text+' '+w\n",
        "    dat.at[i, 'review']=text\n",
        "    # del words\n",
        "  return dat\n",
        "def prepare_text_for_lda(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1Yn_Mak0-nO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}